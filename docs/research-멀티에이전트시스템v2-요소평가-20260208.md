# 멀티에이전트 시스템 v2 - 요소 정의 평가 자료

> 조사일: 2026-02-08

## 개요

멀티에이전트 시스템 v2의 각 요소(에이전트 4개, 스킬 10개, 워크플로우 4개)가 잘 정의되었는지 평가하기 위해, 업계 표준 설계 패턴, Claude Code 공식 스킬/에이전트 문서, 멀티에이전트 평가 프레임워크, 일반적인 실수 사례를 종합 조사했다. 평가 기준을 7개 차원으로 도출하고, 현재 v2 계획서의 각 요소를 이 기준에 대입하여 평가한다.

## 상세 내용

### 1. 요소 평가를 위한 7대 기준

업계 자료와 Anthropic 공식 가이드를 종합하면, 잘 정의된 멀티에이전트 시스템의 요소는 다음 기준을 충족해야 한다:

#### 기준 1: 역할 경계의 명확성 (Role Boundary Clarity)

각 에이전트/스킬이 "무엇을 하는가"와 "무엇을 하지 않는가"가 모두 명시되어야 한다.

- Google의 멀티에이전트 설계 원칙: 에이전트는 AI 마이크로서비스처럼 "분산과 전문화"로 신뢰성을 얻는다
- LangChain 가이드: 도구를 먼저 추가하고, 단일 에이전트의 한계에 부딪힐 때만 멀티에이전트로 확장하라
- 평가 질문: "이 에이전트/스킬을 제거하면, 그 역할을 다른 에이전트/스킬이 대신할 수 있는가? 있다면 존재 근거가 약하다"

#### 기준 2: 입출력 계약의 구체성 (I/O Contract Specificity)

입력 형식, 출력 형식, 성공/실패 조건이 구체적으로 정의되어야 한다.

- Anthropic 에이전트 평가 가이드: 명확한 성공 기준 없이는 평가 자체가 불가능하다
- 도구 정의 품질 연구: 잘 정의된 입출력 스키마와 직관적 이름이 LLM의 올바른 도구 선택을 돕는다
- 평가 질문: "이 스킬의 출력을 다음 단계가 파싱 없이 바로 소비할 수 있는가?"

#### 기준 3: 핸드오프 설계의 완결성 (Handoff Completeness)

에이전트 간, 스킬 간 컨텍스트 전달 방법이 명시되어야 한다.

- 멀티에이전트 실패 원인 연구: 대부분의 에이전트 실패는 오케스트레이션과 컨텍스트 전달 문제에서 발생한다
- Google 8대 패턴: Generator-Critic 패턴에서 피드백 루프의 종료 조건이 없으면 무한 반복에 빠진다
- 평가 질문: "DO에서 REVIEW로 넘어갈 때, 어떤 정보가 전달되는가? REVIEW 결과로 REDO할 때, 어떤 컨텍스트가 보존되는가?"

#### 기준 4: 실패 모드 대응 (Failure Mode Handling)

각 요소의 실패 시나리오와 복구 전략이 정의되어야 한다.

- AI 코딩 에이전트 함정: 자율 에이전트가 오래 실행되면 실수가 누적되어 코드에 고착된다
- Anthropic 하네스 가이드: 조기 완료 선언, 미문서화된 진행, 테스트 공백이 주요 실패 모드이다
- 평가 질문: "REVIEW 에이전트가 잘못된 PASS를 내리면 어떻게 되는가? 스킬이 중간에 실패하면 상태는 어떻게 되는가?"

#### 기준 5: 컨텍스트 효율성 (Context Efficiency)

각 요소가 컨텍스트 윈도우를 얼마나 효율적으로 사용하는가.

- Claude Code 스킬 가이드: SKILL.md는 500줄 이하를 권장하며, 스킬 설명은 컨텍스트 윈도우의 2%까지 차지한다
- 프롬프트 쇠퇴(Prompt Decay) 문제: 프롬프트가 시간이 지나면서 효과를 잃어, 보안 에이전트가 깊이 대신 속도를 우선시하기 시작할 수 있다
- 평가 질문: "이 에이전트/스킬의 프롬프트에서 제거해도 동작에 영향 없는 부분이 있는가?"

#### 기준 6: 차별성 (Differentiation)

같은 입력에 대해 각 에이전트가 실제로 다른 관점의 출력을 생성하는가.

- v2 성공 기준 #1: "architect, reviewer, security가 같은 코드에서 실제로 다른 지적을 하는가?"
- AgentMesh 프레임워크: Planner, Coder, Debugger, Reviewer가 각각 하위 태스크 분해, 구현, 검증, 최종 평가를 담당하며 역할이 겹치지 않는다
- 평가 질문: "같은 코드 변경에 대해 architect와 reviewer의 리뷰 결과가 다른 축을 다루는가?"

#### 기준 7: 테스트 가능성 (Testability)

각 요소를 독립적으로 테스트할 수 있는가.

- Anthropic 평가 프레임워크: 코드 기반 채점(결정론적) + 모델 기반 채점(유연) + 인간 채점(기준)의 3중 평가가 필요하다
- pass@k와 pass^k: 탐색 능력(k번 중 1번 성공)과 신뢰성(k번 모두 성공)을 모두 측정해야 한다
- 평가 질문: "이 에이전트를 단독으로 호출하여, 기대한 형식의 리뷰를 생성하는지 검증할 수 있는가?"

---

### 2. 현재 v2 요소별 평가

#### 2.1 에이전트 평가 (4개)

| 에이전트 | 역할 경계 | I/O 계약 | 핸드오프 | 실패 모드 | 컨텍스트 효율 | 차별성 | 테스트 가능 |
|----------|-----------|----------|----------|-----------|---------------|--------|------------|
| architect | A | A | B | C | A | A | B |
| reviewer | A | A | B | C | A | A | B |
| security | A | A | B | C | A | A | B |
| navigator | A | A | A | C | A | A | A |

**잘 된 점:**
- 4개 에이전트 모두 "What NOT to Review" 섹션이 있어 역할 경계가 명확하다
- Handoff Context 섹션에서 다른 에이전트/스킬과의 연동 방식을 서술한다
- PASS/WARN/BLOCK 3단계 출력 형식이 구체적인 예제와 함께 정의되어 있다
- navigator는 "평가하지 않고 위치만 찾는" 유틸리티로, 다른 3개와 역할이 명확히 구분된다

**개선 필요:**
- **실패 모드 미정의**: 에이전트가 잘못된 판정(False PASS, False BLOCK)을 내렸을 때의 대응이 없다. 예: architect가 심각한 구조 문제를 놓치고 PASS를 내리면?
- **핸드오프 데이터 형식 미정의**: "Task(architect): '~를 리뷰하라'"라는 호출 형식만 있고, 실제로 전달되는 컨텍스트의 구조(파일 목록, diff, 이전 리뷰 결과 등)가 명시되지 않았다
- **모델 선택 재고 필요**: reviewer와 security가 sonnet으로 되어 있었으나 실제 구현에서는 opus로 변경됨. 계획서와 실제 구현이 다르다

#### 2.2 스킬 평가 (10개)

| 스킬 | 역할 경계 | I/O 계약 | 핸드오프 | 실패 모드 | 컨텍스트 효율 | 차별성 | 테스트 가능 |
|------|-----------|----------|----------|-----------|---------------|--------|------------|
| /plan | A | A | A | B | A | A | B |
| /deepresearch | A | A | B | C | A | A | A |
| /decide | A | A | B | B | A | A | A |
| /implement | A | A | A | B | A | A | B |
| /test | A | A | A | A | A | A | A |
| /fix | A | A | A | B | A | A | B |
| /commit | A | A | A | A | A | A | A |
| /security-scan | - | - | - | - | - | - | - |
| /deploy | - | - | - | - | - | - | - |

**잘 된 점:**
- 각 스킬의 프로세스가 단계별로 구체적이다 (번호가 매겨진 절차)
- 출력 형식이 마크다운 템플릿으로 정의되어 있다
- REVIEW 연동 섹션에서 어떤 에이전트에게 리뷰를 요청하는지 명시한다
- /test의 자동 판정 (전체 통과 -> AUTO PROCEED) 로직이 명확하다
- /commit은 리뷰 없이 자동 완료됨을 명시하여 불필요한 오버헤드를 제거했다

**개선 필요:**
- **/security-scan과 /deploy가 아직 미구현**: 계획서에는 있지만 SKILL.md가 없다. hotfix 워크플로우에서 참조하지만 "구현 예정"으로 표시됨
- **/deepresearch의 핸드오프**: 조사 결과를 어떤 형식으로 다른 스킬/에이전트에 전달하는지 불명확하다. /plan에서 호출할 때 결과를 어떻게 소비하는가?
- **/implement의 navigator 호출 후 실패 시**: navigator가 관련 코드를 못 찾으면 어떻게 하는가?
- **스킬 간 중복**: /fix와 /bugfix 워크플로우 사이의 관계가 혼동될 수 있다. /fix는 스킬(실행 단위)이고 /bugfix는 워크플로우(오케스트레이션)이지만, 사용자가 "/fix 해줘"라고 하면 어느 것이 호출되는가?

#### 2.3 워크플로우 평가 (4개)

| 워크플로우 | 역할 경계 | I/O 계약 | 핸드오프 | 실패 모드 | 컨텍스트 효율 | 차별성 | 테스트 가능 |
|-----------|-----------|----------|----------|-----------|---------------|--------|------------|
| /new-project | A | A | A | B | B | A | C |
| /improve | A | A | A | B | A | A | C |
| /bugfix | A | A | A | B | A | A | C |
| /hotfix | A | A | A | B | A | A | C |

**잘 된 점:**
- 4개 워크플로우 모두 "트리거 조건"과 "다른 워크플로우와의 구분"이 명시되어 있다
- DO/REVIEW 시퀀스가 Phase별로 구조화되어 있다
- 종료 조건이 명확하다
- "최대 2회 재시도 후 사용자에게 판단 요청" 같은 에스컬레이션 정책이 있다
- 완료 보고 템플릿이 일관된 형식이다

**개선 필요:**
- **테스트 가능성 부족**: 워크플로우 전체를 end-to-end로 테스트할 방법이 정의되지 않았다. Phase 5 검증 계획은 있지만 구체적인 테스트 시나리오가 없다
- **/new-project의 컨텍스트 소비**: 기획->조사->개발->QA->배포를 하나의 세션에서 실행하면 컨텍스트 윈도우가 부족할 수 있다. context: fork로 서브에이전트에서 실행하지만, 전체 오케스트레이션 상태 관리가 불명확하다
- **미구현 스킬 참조**: /hotfix가 /security-scan을 호출하지만 아직 미구현이다. /new-project가 /deploy를 호출하지만 역시 미구현이다
- **병렬 처리 미활용**: /new-project Phase 3에서 독립적인 태스크를 병렬 실행할 수 있다고 했으나, 구체적인 병렬화 방법이 없다

---

### 3. 업계 표준 대비 비교

#### 3.1 Google 8대 패턴 매핑

| Google 패턴 | v2 적용 여부 | 해당 요소 |
|-------------|-------------|-----------|
| Sequential Pipeline | O | 워크플로우 전체 (Phase 순서) |
| Coordinator/Dispatcher | O | 워크플로우 오케스트레이터 |
| Parallel Fan-out/Gather | 부분 | /deepresearch (병렬 검색) |
| Generator and Critic | O | DO/REVIEW 패턴 핵심 |
| Iterative Refinement | O | BLOCK -> REDO 루프 |
| Human in the Loop | O | 2회 재시도 후 사용자 판단 |
| Hierarchical Decomposition | O | /plan -> /implement (태스크 분해) |
| Composite Pattern | O | 워크플로우 전체 |

v2는 Google의 8대 패턴 중 7개를 적용하고 있다. 특히 DO/REVIEW가 Generator-Critic 패턴을 잘 반영한다.

#### 3.2 AgentMesh 프레임워크 대비

| AgentMesh | v2 대응 |
|-----------|---------|
| Planner | /plan 스킬 |
| Coder | /implement 스킬 |
| Debugger | /fix 스킬 + /test 스킬 |
| Reviewer | reviewer 에이전트 |
| Controller | 워크플로우 오케스트레이터 |

v2는 AgentMesh보다 리뷰 관점을 세분화했다 (reviewer + architect + security). 이는 DO/REVIEW 원칙에 부합하며, 단일 Reviewer보다 다각적 평가가 가능하다.

#### 3.3 Claude Code 공식 가이드 준수 여부

| 공식 권장사항 | v2 준수 | 비고 |
|--------------|---------|------|
| SKILL.md 500줄 이하 | O | 대부분 100줄 이내 |
| context: fork 사용 | O | 모든 스킬에 적용 |
| allowed-tools 명시 | O | 최소 권한 원칙 적용 |
| description에 트리거 키워드 | O | 한국어/영어 키워드 모두 포함 |
| What NOT to Review 명시 | O | 에이전트 파일에 포함 |
| 지원 파일 분리 | X | 별도 지원 파일 없음 (현재 불필요) |
| argument-hint 사용 | O | 모든 스킬에 포함 |

---

### 4. 발견된 개선 사항 요약

#### 즉시 수정 필요 (우선순위 높음)

1. **에이전트 핸드오프 데이터 형식 명세**: Task() 호출 시 전달되는 컨텍스트 구조를 정의해야 한다. 예: `Task(reviewer): { files: [...], diff: "...", previous_review: null }`
2. **스킬 간 트리거 충돌 해소**: /fix(스킬)와 /bugfix(워크플로우)의 트리거 조건 중복을 해소해야 한다. 사용자가 "고쳐줘"라고 하면 어떤 것이 실행되는가?
3. **계획서와 구현의 불일치 해소**: reviewer/security 모델이 계획서(sonnet)와 실제 구현(opus)이 다르다

#### 구현 시 주의 (우선순위 중간)

4. **실패 모드 정의 추가**: 각 에이전트에 "잘못된 판정 시 대응" 섹션 추가
5. **/security-scan, /deploy 구현**: 워크플로우에서 참조하지만 미구현 상태
6. **워크플로우 테스트 시나리오 구체화**: Phase 5 검증을 위한 구체적 테스트 케이스 작성
7. **컨텍스트 윈도우 관리 전략**: /new-project 같은 대형 워크플로우의 세션 분할 전략

#### 장기 개선 (우선순위 낮음)

8. **프롬프트 쇠퇴 모니터링**: 시간이 지나면서 에이전트 리뷰 품질이 떨어지는지 주기적으로 검증하는 체계
9. **메트릭 수집**: pass@k, pass^k 등 정량적 평가 지표 도입
10. **병렬 실행 최적화**: /new-project Phase 3에서의 독립 태스크 병렬 실행 구체화

## 코드 예제

### 핸드오프 데이터 형식 명세 (개선안)

현재 에이전트 호출은 자연어 프롬프트만 전달한다:
```
Task(reviewer): "다음 파일들의 코드 품질을 리뷰하라: src/auth.ts"
```

구조화된 핸드오프로 개선하면:
```
Task(reviewer): """
## Review Request

**Source**: /implement skill
**Scope**: src/auth.ts (new, 87 lines), src/routes/login.ts (modified, 12 lines changed)
**Context**: 로그인 기능 구현. JWT 기반 인증.
**Previous Reviews**: none
**Focus**: 코드 품질, 기존 패턴 준수

변경된 파일 diff:
[diff 내용]
"""
```

### 트리거 충돌 해소 (개선안)

```yaml
# /fix 스킬 - description 수정
description: |
  [단일 버그 수정 실행 단위]
  Diagnoses and patches a single bug with regression tests.
  Called by /bugfix workflow or directly for quick fixes.
  Triggers on: 직접 호출 또는 워크플로우에서 호출
  Note: 일반적으로 사용자는 /bugfix 워크플로우를 사용하며,
  /fix는 워크플로우 내부에서 자동 호출된다.
disable-model-invocation: true  # 워크플로우를 통해서만 호출
```

## 주요 포인트

1. **7대 평가 기준 도출**: 역할 경계, I/O 계약, 핸드오프, 실패 모드, 컨텍스트 효율, 차별성, 테스트 가능성 - 이 기준으로 각 요소를 체계적으로 평가할 수 있다

2. **v2의 강점**: Google 8대 패턴 중 7개를 자연스럽게 적용하고 있으며, DO/REVIEW 패턴이 Generator-Critic을 잘 구현했다. 에이전트별 "What NOT to Review" 정의와 PASS/WARN/BLOCK 출력 형식이 업계 권장사항에 부합한다

3. **핵심 약점 3가지**: (1) 에이전트 핸드오프 시 전달되는 데이터 형식이 미정의, (2) /fix 스킬과 /bugfix 워크플로우의 트리거 충돌 가능성, (3) 실패 모드 대응(잘못된 판정, 스킬 중간 실패)이 미정의

4. **미구현 의존성**: /security-scan과 /deploy가 아직 없어 /hotfix와 /new-project 워크플로우가 불완전하다. Phase 4 구현까지는 워크플로우에서 해당 단계를 건너뛰는 것이 명시되어 있어 치명적이지는 않다

5. **전체 평가**: 현재 v2는 "잘 정의된 편"이다. 에이전트 역할 분리, 스킬 프로세스 구체성, 워크플로우 구조화 수준이 업계 표준 이상이다. 주요 개선점은 "경계 조건"(핸드오프 데이터, 실패 모드, 트리거 충돌)에 집중되어 있으며, 핵심 설계 원칙 자체는 건전하다

## 출처

- [Google's Eight Essential Multi-Agent Design Patterns - InfoQ](https://www.infoq.com/news/2026/01/multi-agent-design-patterns/)
- [Choosing the Right Multi-Agent Architecture - LangChain](https://blog.langchain.com/choosing-the-right-multi-agent-architecture/)
- [Demystifying Evals for AI Agents - Anthropic](https://www.anthropic.com/engineering/demystifying-evals-for-ai-agents)
- [Effective Harnesses for Long-Running Agents - Anthropic](https://www.anthropic.com/engineering/effective-harnesses-for-long-running-agents)
- [Extend Claude with Skills - Claude Code Docs](https://code.claude.com/docs/en/skills)
- [Claude Agent Skills: A First Principles Deep Dive](https://leehanchung.github.io/blogs/2025/10/26/claude-skills-deep-dive/)
- [Claude Code's Hidden Multi-Agent System](https://paddo.dev/blog/claude-code-hidden-swarm/)
- [AI Coding Agents: Pain Points, Pitfalls & Pro Tips](https://www.smiansh.com/blogs/the-real-struggle-with-ai-coding-agents-and-how-to-overcome-it/)
- [How to Build Multi-Agent Systems: Complete 2026 Guide - DEV](https://dev.to/eira-wexford/how-to-build-multi-agent-systems-complete-2026-guide-1io6)
- [AgentMesh: A Cooperative Multi-Agent Framework](https://arxiv.org/html/2507.19902v1)
- [AI Agent Orchestration Patterns - Azure Architecture Center](https://learn.microsoft.com/en-us/azure/architecture/ai-ml/guide/ai-agent-design-patterns)
- [A Comprehensive Guide to Evaluating Multi-Agent LLM Systems - orq.ai](https://orq.ai/blog/multi-agent-llm-eval-system)
- [Building Agents with the Claude Agent SDK - Anthropic](https://www.anthropic.com/engineering/building-agents-with-the-claude-agent-sdk)
